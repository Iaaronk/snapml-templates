{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9APTDMy1TiO"
   },
   "source": [
    "## Introduction\n",
    "Optical Character Recognition (OCR) is critical in Computer Vision and Artificial Intelligence. Its applications span a variety of sectors, including document digitization, automated data entry, and license plate recognition. Unlike typical image processing tasks, OCR identifies, extracts, and digitizes written or printed characters from images or documents. This technology bridges the gap between the physical text world and the digital realm, allowing computers to understand and utilize written text within images.\n",
    "\n",
    "This guide will show you how to run a powerful OCR model Paddle-OCR and deploy it using SnapML. You can read more about PaddleOCR in their official repo [here](https://github.com/PaddlePaddle/PaddleOCR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktf3w-B424rJ"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axUkzxUi7nGT"
   },
   "source": [
    "### Python Inference\n",
    "To test the model in our Python framework and convert it into a SnapML-compatible ONNX graph, we've prepared [this](https://github.com/opencv-ai/paddle-ocr) repository. You need to clone it and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2999,
     "status": "ok",
     "timestamp": 1690377176471,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "ORLnIhMG9dqY",
    "outputId": "316c5bde-7d0e-4cfc-f0c1-e32b93a9d503"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/opencv-ai/paddle-ocr\n",
    "!cd paddle-ocr && pip install -r requirements.txt\n",
    "!pip install git+https://github.com/daquexian/onnx-simplifier.git@v0.3.6 numpy==1.21.6 paddle2onnx gdown paddlepaddle==2.4.2 Pillow==9.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIOoMw5g904i"
   },
   "source": [
    "## Training (optional)\n",
    "\n",
    "We've prepared a separate notebook with instructions on launching the PaddleOCR training scripts. We didn't train the model on our side and reproduce the authors' instructions there; therefore, we can't guarantee that the results will match the authors' pre-trained model we used. The training notebook can be found [here](https://drive.google.com/file/d/1K_QAvqBF-lzHtDZrHLYwjbbPj0UR0dVn/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHrilj060QYr"
   },
   "source": [
    "## Export\n",
    "\n",
    "We need to convert the model weights into ONNX format to run our Python pipeline and use it with SnapML. We provide converted models in our Python repository, and you may skip these steps if you want to.\n",
    "\n",
    "We used the pre-trained checkpoints. You need to download it or train your own models. To prepare your own weights for convertation, follow [the instructions](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/doc/doc_en/inference_en.md#1-convert-training-model-to-inference-model) to save inference state checkpoint.\n",
    "\n",
    "Launch the cell below to download pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33800,
     "status": "ok",
     "timestamp": 1690377093776,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "NC2yUNCe0QYr",
    "outputId": "eb99cdb1-e4fa-4f64-9150-9294c204cb90"
   },
   "outputs": [],
   "source": [
    "%cd paddle-ocr\n",
    "!mkdir paddle_weights\n",
    "\n",
    "!wget -P ./paddle_weights/ https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\n",
    "!cd paddle_weights && tar -xf en_PP-OCRv3_det_infer.tar && rm -rf en_PP-OCRv3_det_infer.tar\n",
    "!wget -P ./paddle_weights/ https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/en_number_mobile_v2.0_rec_infer.tar\n",
    "!cd paddle_weights && tar -xf en_number_mobile_v2.0_rec_infer.tar && rm -rf en_number_mobile_v2.0_rec_infer.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgexoUMR0QYr"
   },
   "source": [
    "Now you need to convert the weights into ONNX using Paddle2ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6092,
     "status": "ok",
     "timestamp": 1690377279491,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "W4D7XLdY0QYs",
    "outputId": "02d83bc6-c294-4a3b-c6ae-e95a1e7c9d30"
   },
   "outputs": [],
   "source": [
    "!paddle2onnx --model_dir ./paddle_weights/en_PP-OCRv3_det_infer \\\n",
    "             --model_filename inference.pdmodel \\\n",
    "             --params_filename inference.pdiparams\\\n",
    "             --save_file ./weights/source/en_PP-OCRv3_det_infer_fixed_shape.onnx \\\n",
    "             --enable_dev_version False \\\n",
    "             --input_shape_dict \"{'x':[1,3,640,640]}\" \\\n",
    "             --opset_version 11 \\\n",
    "             --enable_onnx_checker True\n",
    "\n",
    "!paddle2onnx --model_dir ./paddle_weights/en_number_mobile_v2.0_rec_infer \\\n",
    "             --model_filename inference.pdmodel \\\n",
    "             --params_filename inference.pdiparams\\\n",
    "             --save_file ./weights/source/en_number_mobile_v2.0_rec_infer_fixed_shape.onnx \\\n",
    "             --enable_dev_version False \\\n",
    "             --input_shape_dict \"{'x':[1,3,32,832]}\" \\\n",
    "             --opset_version 11 \\\n",
    "             --enable_onnx_checker True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xXVxm3w0QYs"
   },
   "source": [
    "Let's simplify our models using onnxsim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9830,
     "status": "ok",
     "timestamp": 1690377346674,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "eX-P3vlp0QYs",
    "outputId": "bb9da783-a16c-487d-d26d-e2dd5ea87fb0"
   },
   "outputs": [],
   "source": [
    "!python -m onnxsim ./weights/source/en_PP-OCRv3_det_infer_fixed_shape.onnx ./weights/source/en_PP-OCRv3_det_infer_fixed_shape_optimized.onnx\n",
    "!python -m onnxsim ./weights/source/en_number_mobile_v2.0_rec_infer_fixed_shape.onnx ./weights/source/en_number_mobile_v2.0_rec_infer_fixed_shape_optimized.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XL0lGTT0QYs"
   },
   "source": [
    "As the final step, we need to correct some operations to make the model compatible with SnapML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1463,
     "status": "ok",
     "timestamp": 1690377355753,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "r9LOzDWe0QYs",
    "outputId": "f49e477f-cbe7-4ee1-f9b5-14ad8d77cdd6"
   },
   "outputs": [],
   "source": [
    "!python ./weights/fix_detector.py --input_path ./weights/source/en_PP-OCRv3_det_infer_fixed_shape_optimized.onnx --output_path ./weights/changed/detector_v3.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1690377386790,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "lrR7olBJ0QYs",
    "outputId": "a77094f5-e041-46bf-8529-7c20560d780d"
   },
   "outputs": [],
   "source": [
    "!python ./weights/fix_recognizer.py --input_path ./weights/source/en_number_mobile_v2.0_rec_infer_fixed_shape_optimized.onnx --output_path ./weights/changed/recognition_v1.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLQopDj_0QYt"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Let's run our ONNX pipeline on some images. We support a text detection pipeline (use `--run_detection` argument) or an entire pipeline with recognition (`--run_pipeline`). Note that these arguments conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1690377391792,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "PnY_VoBn0QYt",
    "outputId": "216c8861-7500-4441-dd5b-c3e3ebc36f2a"
   },
   "outputs": [],
   "source": [
    "!python inference.py --det_model_dir ./weights/changed/detector_v3.onnx \\\n",
    "                     --rec_model_dir ./weights/changed/recognition_v1.onnx \\\n",
    "                     -i ./images/test2.png \\\n",
    "                     --run_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843,
     "output_embedded_package_id": "1JVVwWb4eogEDAUtjrL1ByufgkxuAB3Zg"
    },
    "executionInfo": {
     "elapsed": 2150,
     "status": "ok",
     "timestamp": 1690377397246,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "YFXThvvQ0QY0",
    "outputId": "94453b36-e6b7-4f41-e866-e6af1f9011c1"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"./images/results/test2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1721,
     "status": "ok",
     "timestamp": 1690377402200,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "FeZvbnqh0QY0",
    "outputId": "6f625335-f6a6-4a44-dcca-f67bd580cdfe"
   },
   "outputs": [],
   "source": [
    "!python inference.py --det_model_dir ./weights/changed/detector_v3.onnx \\\n",
    "                     --rec_model_dir ./weights/changed/recognition_v1.onnx \\\n",
    "                     -i ./images/test1.png \\\n",
    "                     --run_pipeline \\\n",
    "                     --rec_char_dict_path ./rec_char_dict/en_dict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1690377404255,
     "user": {
      "displayName": "Pavel Semkin",
      "userId": "04453955975692038114"
     },
     "user_tz": -180
    },
    "id": "5bdUoaxx0QY0",
    "outputId": "6653006d-4013-4123-efa1-a51732ea1763"
   },
   "outputs": [],
   "source": [
    "Image.open(\"./images/results/test1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations\n",
    "\n",
    "- Works well on full frontal views with very little obstruction in text, where text is horizontally placed. License plates are a good example where it works well, front facing billboards, book covers with horizontal text are good too. \n",
    "- Doesn’t work well on use cases where the text is slanted (for example road signs that may be angled, or a book cover that is placed on a table on an angle etc).\n",
    "- Text that is very exaggerated or highly stylized is also one where it doesn’t work very well. If there are ornaments near the text, it tends to capture those as well and try to recognize them as special characters. \n",
    "- Sometimes, the model is not able to recognize spaces, leading to confusing outputs.\n",
    "- Big chunks of small text are also a hit or miss.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
