{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2L5nirGfR_f"
      },
      "source": [
        "# Introduction\n",
        "Neural rendering is an emerging but powerfull technique for photo-realistic novel view synthesis from 2D images. Its applications span from object digitalization, 3D assets generation to Virtual Reality and Augmented Reality. Creating 3D assets, in general, requires expertise in various fields that is not easily accessible to individuals. However, with this techology, one can build their own AR assets from the images taken from their phones.\n",
        "\n",
        "Specifically, in this guide, we will guide you through the process of building your own AR assets which is fully compatible with SnapML and can be running on devices in real-time, strating from training the models to deploying the lens.\n",
        "\n",
        "\n",
        "# Setup the AWS\n",
        "Follow this [tutorial](https://docs.google.com/document/d/1nqCN6xZDXb_6o8rOZ0RNTydfL3ZF05fSCP03xlUJR-I/edit#heading=h.jfsq857ofbwv) on how to setup AWS for training.\n",
        "\n",
        "# Setup the Environment\n",
        "There are two major dependencies to install: [NGP_PL](https://github.com/kwea123/ngp_pl) and [MobileR2L](https://snap-research.github.io/MobileR2L/). `NGP_PL` is a Pytorch implementation of [Instant NGP](https://nvlabs.github.io/instant-ngp/) which serves as the teacher of `MobileR2L`. We will walk you through each of the installation below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drGBAYbDivDZ"
      },
      "source": [
        "#### MobileR2L\n",
        "Create a `Conda` environment and install the dependencies as below. We use `torch 2.0.1+cu11.7` as default version but this codebase is also compatiable with `torch 1.13.1+cu116`(training is slower!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYMs7QlyxODf"
      },
      "outputs": [],
      "source": [
        "# get the code\n",
        "!git clone https://github.com/snap-research/MobileR2L\n",
        "\n",
        "%cd MobileR2L\n",
        "\n",
        "# create conda env\n",
        "# download conda here: https://docs.conda.io/en/main/miniconda.html\n",
        "!conda create -n r2l python==3.9\n",
        "!conda activate r2l\n",
        "!conda install pip\n",
        "\n",
        "# install dependencies.\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# deactivate the env for now\n",
        "!conda deactivate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrXmO6Iji0y4"
      },
      "source": [
        "#### NGP_PL\n",
        "Let's first clone the repo and create a `Conda` environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ3l37CWiv5a"
      },
      "outputs": [],
      "source": [
        "# change to the teacher directory\n",
        "%cd model/teacher/ngp_pl\n",
        "\n",
        "# create the conda env\n",
        "!conda create -n ngp_pl python==3.9\n",
        "!conda activate ngp_pl\n",
        "!conda install pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uadEgGf2jo_2"
      },
      "source": [
        "Next, let's install the dependencies for the `NGP_PL`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lsOHuHLjvqB"
      },
      "outputs": [],
      "source": [
        "# install torch with cuda 116\n",
        "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "# install tiny-cuda-nn\n",
        "!pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\n",
        "\n",
        "# install torch scatter\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.0+${cu116}.html\n",
        "\n",
        "# ---install apex---\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!cd apex\n",
        "# denpency for apex\n",
        "!pip install packaging\n",
        "\n",
        "## if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
        "## otherwise\n",
        "# !pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "# ---end installing apex---\n",
        "\n",
        "\n",
        "%cd ../\n",
        "# install other requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# build\n",
        "!pip install models/csrc/\n",
        "\n",
        "# go to root\n",
        "%cd ../../../\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvyHwZygkyAf"
      },
      "source": [
        "# Dataset\n",
        "There are two major types of datasets for static NeRF: object-centric and forward-facing scenes. In this guide, we will be using object-centric as the example. Specifically, we use `Lego` scene from `NeRF Synthetic Dataset` [here](https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6bP4NSlkxS6"
      },
      "outputs": [],
      "source": [
        "# get the data\n",
        "!mkdir -p dataset\n",
        "%cd dataset\n",
        "# it downloads Lego and Fern\n",
        "!wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_example_data.zip\n",
        "!unzip nerf_example_data.zip\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DpSkEnTrZDM"
      },
      "source": [
        "# Training the Teacher\n",
        "Now we have the data ready and we can start to train the teacher model. First, let's go to the teacher directory. Then run the train script. Once the training finished, the checkpoint will be save to `ckpts/nerf/Lego`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PuWs7FOrteE",
        "outputId": "0832fb22-fc25-4892-9a6a-df9dea50fe50"
      },
      "outputs": [],
      "source": [
        "%cd model/teacher/ngp_pl\n",
        "\n",
        "!export ROOT_DIR=../../../dataset/nerf_synthetic/\n",
        "!python3 train.py \\\n",
        "     --root_dir $ROOT_DIR/lego \\\n",
        "     --exp_name Lego\\\n",
        "     --num_epochs 30 --batch_size 16384 --lr 2e-2 --eval_lpips --num_gpu 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP4_H53wtUX3"
      },
      "source": [
        "Once we have the teacher trained(checkpoints saved already), we can start to generate the pseudo data for `MobileR2L`. Depending your disk storage, the number of pseudo images could range from 2,000 to 10,000(performance varies!). Here, we set the number to 3000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYgMjzc9rw-Z"
      },
      "outputs": [],
      "source": [
        "!python3 train.py \\\n",
        "    --root_dir $ROOT_DIR/lego \\\n",
        "    --exp_name Lego_Pseudo  \\\n",
        "    --save_pseudo_data \\\n",
        "    --n_pseudo_data 3000 --weight_path ckpts/nerf/Lego/epoch=29_slim.ckpt \\\n",
        "    --save_pseudo_path Pseudo/lego --num_gpu 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu9tR9gtvqC7"
      },
      "source": [
        "# Training the MobileR2L\n",
        "We suggest to use ternimal with [tmux](https://github.com/tmux/tmux/wiki) instead of running it in the notebook as it takes longer to converge. Notebook might be interruppted in the middle of training. To use the terminal, simply copy the command and run it in therminal(without % and !)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n79SqvLHwagf"
      },
      "outputs": [],
      "source": [
        "# change to root directory\n",
        "%cd ../../../\n",
        "\n",
        "# use r2l env\n",
        "!conda activate r2l\n",
        "\n",
        "# install the tmux\n",
        "!sudo apt install tmux\n",
        "# create new session\n",
        "!tmux new -s lego\n",
        "\n",
        "# run the bash\n",
        "!sh benchmarking_nerf.sh\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmuAl5lO0AlM"
      },
      "source": [
        "The model will be running a day or two depending on you GPUs. When the model converges, it will automatically export the onnx files to the `Experiment/Lego_**` folder. There should be three onnx files: `sampler`, `embedder` and `model`.\n",
        "\n",
        "Alternatively, you can export the onnx manully by running the flowing script with `--ckpt_dir` replaced by the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQfNtSyRlf_P"
      },
      "outputs": [],
      "source": [
        "!python3 -m torch.distributed.launch --nproc_per_node=1 --use_env main.py \\\n",
        "      --project_name Lego \\\n",
        "      --dataset_type Blender \\\n",
        "      --pseudo_dir model/teacher/ngp_pl/Pseudo/lego \\\n",
        "      --root_dir dataset/nerf_synthetic \\\n",
        "      --export_onnx \\\n",
        "      --input_height 100 \\\n",
        "      --input_width 100 \\\n",
        "      --output_height 800 \\\n",
        "      --output_width 800 \\\n",
        "      --scene lego \\\n",
        "      --ckpt_dir Experiments/Lego-**/weights/best_ckpt.tar\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Lens Projects\n",
        "\n",
        "We provide two lens projects: one supports 360 degree rotation with fingers([here](https://drive.google.com/file/d/1GpbLVZ4n0HMk72im4dK4EgrjygVADTHN/view?usp=drive_link)); The other one adds surface tracking feature([here](https://drive.google.com/file/d/1FMXMhTyIRS-J_wMM7Pd7fts9vBRzq4Ay/view?usp=drive_link)). Both of the projects include some pre-built models that you can directly scan the snapcodes and run on your phones(iPhone 13 or higher is recommended).\n",
        "\n",
        "If you wish to use your own trained model in the lens, simply import the `*_SnapGELU.onnx` to Lens Studio(need to scale the Channel 0-3 to 255.0) and \n",
        "1. for `360 degree rotation lens`: swap out the onnx file in the **first** `MLComponent` under `camera` in the left pannel of LS .\n",
        "2. for `sufrace tracking lens`: swap out the onnx file in `NeRF MLComponent`.\n",
        "\n",
        "> Figure: 360 degree rotation lens\n",
        "> ![](ml-comp.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
